{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, Conv2DTranspose, BatchNormalization, Dropout, Lambda\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "################################################################\n",
    "def multi_unet_model(n_classes=2, IMG_HEIGHT= 512, IMG_WIDTH= 512, IMG_CHANNELS=1):\n",
    "#Build the model\n",
    "    inputs = Input((IMG_HEIGHT, IMG_WIDTH, IMG_CHANNELS))\n",
    "    #s = Lambda(lambda x: x / 255)(inputs)   #No need for this if we normalize our inputs beforehand\n",
    "    s = inputs\n",
    "    \n",
    "    #Contraction path\n",
    "    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(s)\n",
    "    c1 = Dropout(0.1)(c1)\n",
    "    c1 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c1)\n",
    "    p1 = MaxPooling2D((2, 2))(c1)\n",
    "    \n",
    "    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p1)\n",
    "    c2 = Dropout(0.1)(c2)\n",
    "    c2 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c2)\n",
    "    p2 = MaxPooling2D((2, 2))(c2)\n",
    "     \n",
    "    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p2)\n",
    "    c3 = Dropout(0.2)(c3)\n",
    "    c3 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c3)\n",
    "    p3 = MaxPooling2D((2, 2))(c3)\n",
    "     \n",
    "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p3)\n",
    "    c4 = Dropout(0.2)(c4)\n",
    "    c4 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c4)\n",
    "    p4 = MaxPooling2D(pool_size=(2, 2))(c4)\n",
    "     \n",
    "    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(p4)\n",
    "    c5 = Dropout(0.3)(c5)\n",
    "    c5 = Conv2D(256, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c5)\n",
    "    \n",
    "    #Expansive path \n",
    "    u6 = Conv2DTranspose(128, (2, 2), strides=(2, 2), padding='same')(c5)\n",
    "    u6 = concatenate([u6, c4])\n",
    "    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u6)\n",
    "    c6 = Dropout(0.2)(c6)\n",
    "    c6 = Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c6)\n",
    "     \n",
    "    u7 = Conv2DTranspose(64, (2, 2), strides=(2, 2), padding='same')(c6)\n",
    "    u7 = concatenate([u7, c3])\n",
    "    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u7)\n",
    "    c7 = Dropout(0.2)(c7)\n",
    "    c7 = Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c7)\n",
    "     \n",
    "    u8 = Conv2DTranspose(32, (2, 2), strides=(2, 2), padding='same')(c7)\n",
    "    u8 = concatenate([u8, c2])\n",
    "    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u8)\n",
    "    c8 = Dropout(0.1)(c8)\n",
    "    c8 = Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c8)\n",
    "     \n",
    "    u9 = Conv2DTranspose(16, (2, 2), strides=(2, 2), padding='same')(c8)\n",
    "    u9 = concatenate([u9, c1], axis=3)\n",
    "    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(u9)\n",
    "    c9 = Dropout(0.1)(c9)\n",
    "    c9 = Conv2D(16, (3, 3), activation='relu', kernel_initializer='he_normal', padding='same')(c9)\n",
    "     \n",
    "    outputs = Conv2D(n_classes, (1, 1), activation='softmax')(c9)\n",
    "     \n",
    "    model = Model(inputs=[inputs], outputs=[outputs])\n",
    "    \n",
    "    #NOTE: Compile the model in the main program to make it easy to test with various loss functions\n",
    "    #model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    #model.summary()\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from CNN_model_segmentation import multi_unet_model #Uses softmax \n",
    "#from CNN_model_segmentation.py import*\n",
    "from tensorflow.keras.utils import normalize\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import callbacks\n",
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resizing images, if needed\n",
    "SIZE_X = 500 \n",
    "SIZE_Y = 500\n",
    "n_classes=2 #Number of classes for segmentation\n",
    "\n",
    "#Capture training image info as a list\n",
    "train_images = []\n",
    "\n",
    "for directory_path in glob.glob(\"D:\\\\01_Thesis\\\\learning\\\\enpal\\\\val\\\\image_500\\\\New_folder\"):\n",
    "    for img_path in glob.glob(os.path.join(directory_path, \"*.tif\"))[0:800]:\n",
    "        img = cv2.imread(img_path, 1)       \n",
    "#         img = cv2.resize(img, (SIZE_Y, SIZE_X), interpolation = cv2.INTER_NEAREST)\n",
    "        img = np.pad(img,((6,6), (6,6), (0,0)),\"constant\",constant_values=(0))\n",
    "        train_images.append(img)\n",
    "       \n",
    "## Convert list to array for machine learning processing        \n",
    "train_images = np.array(train_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# try_img = train_images[0]\n",
    "\n",
    "# def pad_with(vector, pad_width, iaxis, kwargs):\n",
    "#     pad_value = kwargs.get('padder', 10)\n",
    "#     vector[:pad_width[0]] = pad_value\n",
    "#     vector[-pad_width[1]:] = pad_value\n",
    "    \n",
    "# padded = np.pad(try_img,((103,103), (3,3), (0,0)),\"constant\",constant_values=(0))\n",
    "# padded.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 512, 512, 3)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "uniques in train masks before [  0 255]\n"
     ]
    }
   ],
   "source": [
    "#Capture mask/label info as a list\n",
    "train_masks = [] \n",
    "for directory_path in glob.glob(\"D:\\\\01_Thesis\\\\learning\\\\enpal\\\\val\\\\mask_500\\\\New_folder\"):\n",
    "    for mask_path in glob.glob(os.path.join(directory_path, \"*.tif\"))[0:800]:\n",
    "        mask = cv2.imread(mask_path, 0)       \n",
    "#         mask = cv2.resize(mask, (SIZE_Y, SIZE_X), interpolation = cv2.INTER_NEAREST)  #Otherwise ground truth changes due to interpolation\n",
    "        mask = np.pad(mask,((6,6), (6,6)),\"constant\",constant_values=(0))\n",
    "        train_masks.append(mask)\n",
    "\n",
    "print(\"uniques in train masks before\", np.unique(train_masks))\n",
    "#Convert list to array for machine learning processing          \n",
    "train_masks = np.array(train_masks)\n",
    "\n",
    "# background class\n",
    "# train_masks[train_masks < 20] = 0\n",
    "# # train_masks[train_masks < 150 & train_masks > 50] = 1\n",
    "# #Class 1 border\n",
    "# mask = np.logical_and(train_masks < 80, train_masks > 19)\n",
    "# train_masks[mask] = 1\n",
    "\n",
    "# #Class 1 fill\n",
    "# mask = np.logical_and(train_masks < 150, train_masks > 79)\n",
    "# train_masks[mask] = 2\n",
    "\n",
    "# #Class 1 border\n",
    "# mask = np.logical_and(train_masks < 225, train_masks > 149)\n",
    "# train_masks[mask] = 3\n",
    "\n",
    "# #Class 2 fill\n",
    "# train_masks[train_masks >= 225] = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0 255]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(800, 512, 512)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.unique(train_masks))\n",
    "train_masks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(209715200, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bakthavatchalam\\Anaconda3\\envs\\yolo\\lib\\site-packages\\sklearn\\preprocessing\\_label.py:115: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###############################################\n",
    "#Encode labels... but multi dim array so need to flatten, encode and reshape\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "labelencoder = LabelEncoder()\n",
    "n, h, w = train_masks.shape\n",
    "train_masks_reshaped = train_masks.reshape(-1,1)\n",
    "print(train_masks_reshaped.shape)\n",
    "#x = labelencoder.fit(train_masks_reshaped)\n",
    "#train_masks_reshaped_encoded = labelencoder.transform(x)\n",
    "train_masks_reshaped_encoded = labelencoder.fit_transform(train_masks_reshaped)\n",
    "\n",
    "train_masks_encoded_original_shape = train_masks_reshaped_encoded.reshape(n, h, w)\n",
    "\n",
    "\n",
    "np.unique(train_masks_encoded_original_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keys = labelencoder.classes_\n",
    "#values = labelencoder.transform(keys)\n",
    "#dic = dict(zip(keys,values))\n",
    "#print(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "train_images = np.expand_dims(train_images, axis=-1)\n",
    "train_images = normalize(train_images, axis=1)\n",
    "\n",
    "train_masks_input = np.expand_dims(train_masks_encoded_original_shape, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train images shape =  (800, 512, 512, 3, 1) \n",
      " train masks shape = (800, 512, 512, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"train images shape = \",train_images.shape,\"\\n\",\"train masks shape =\", train_masks_input.shape )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a subset of data for quick testing\n",
    "#Picking 10% for testing and remaining for training\n",
    "from sklearn.model_selection import train_test_split\n",
    "X1, X_test, y1, y_test = train_test_split(train_images, train_masks_input, test_size = 0.10, random_state = 0)\n",
    "\n",
    "train_images = []\n",
    "train_masks_input = []\n",
    "\n",
    "train_masks = []\n",
    "train_masks_reshaped = []\n",
    "train_masks_reshaped_encoded = []\n",
    "train_masks_encoded_original_shape = []\n",
    "\n",
    "#Further split training data t a smaller subset for quick testing of models\n",
    "X_train, X_do_not_use, y_train, y_do_not_use = train_test_split(X1, y1, test_size = 0.2, random_state = 0)\n",
    "X1 = []\n",
    "y1 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class values in the dataset are ...  [0 1]\n"
     ]
    }
   ],
   "source": [
    "print(\"Class values in the dataset are ... \", np.unique(y_train))  # 0 is the background/few unlabeled \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "train_masks_cat = to_categorical(y_train, num_classes=n_classes)\n",
    "y_train_cat = train_masks_cat.reshape((y_train.shape[0], y_train.shape[1], y_train.shape[2], n_classes))\n",
    "\n",
    "\n",
    "test_masks_cat = to_categorical(y_test, num_classes=n_classes)\n",
    "y_test_cat = test_masks_cat.reshape((y_test.shape[0], y_test.shape[1], y_test.shape[2], n_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train masks cat = (576, 512, 512, 2)\n",
      "y_train masks cat = (576, 512, 512, 2)\n",
      "test masks cat = (80, 512, 512, 2)\n",
      "y_train masks cat = (80, 512, 512, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"train masks cat =\",train_masks_cat.shape )\n",
    "print(\"y_train masks cat =\",y_train_cat.shape )\n",
    "print(\"test masks cat =\",test_masks_cat.shape )\n",
    "print(\"y_train masks cat =\",y_test_cat.shape )\n",
    "\n",
    "\n",
    "\n",
    "train_masks_cat = []\n",
    "test_masks_cat = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Sanity check\n",
    "import random\n",
    "import numpy as np\n",
    "image_number = random.randint(0, len(X_train))\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(121)\n",
    "plt.imshow(np.reshape(X_train[image_number]*50, (512, 512,3)))\n",
    "plt.subplot(122)\n",
    "plt.imshow(np.reshape(y_train[image_number], (512, 512))*125, cmap='gray')\n",
    "plt.show()\n",
    "#print(X_train[image_number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "compute_class_weight() takes 1 positional argument but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m###############################################################\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m class_weight\n\u001b[1;32m----> 3\u001b[0m class_weights \u001b[38;5;241m=\u001b[39m \u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_class_weight\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbalanced\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                                                 \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munique\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_masks_reshaped_encoded\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                                                 \u001b[49m\u001b[43mtrain_masks_reshaped_encoded\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass weights are...:\u001b[39m\u001b[38;5;124m\"\u001b[39m, class_weights)\n",
      "\u001b[1;31mTypeError\u001b[0m: compute_class_weight() takes 1 positional argument but 3 were given"
     ]
    }
   ],
   "source": [
    "###############################################################\n",
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 np.unique(train_masks_reshaped_encoded),\n",
    "                                                 train_masks_reshaped_encoded)\n",
    "print(\"Class weights are...:\", class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 512, 512, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d_19 (Conv2D)             (None, 512, 512, 16  448         ['input_2[0][0]']                \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_9 (Dropout)            (None, 512, 512, 16  0           ['conv2d_19[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_20 (Conv2D)             (None, 512, 512, 16  2320        ['dropout_9[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_4 (MaxPooling2D)  (None, 256, 256, 16  0          ['conv2d_20[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_21 (Conv2D)             (None, 256, 256, 32  4640        ['max_pooling2d_4[0][0]']        \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_10 (Dropout)           (None, 256, 256, 32  0           ['conv2d_21[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_22 (Conv2D)             (None, 256, 256, 32  9248        ['dropout_10[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_5 (MaxPooling2D)  (None, 128, 128, 32  0          ['conv2d_22[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_23 (Conv2D)             (None, 128, 128, 64  18496       ['max_pooling2d_5[0][0]']        \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_11 (Dropout)           (None, 128, 128, 64  0           ['conv2d_23[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_24 (Conv2D)             (None, 128, 128, 64  36928       ['dropout_11[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d_6 (MaxPooling2D)  (None, 64, 64, 64)  0           ['conv2d_24[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_25 (Conv2D)             (None, 64, 64, 128)  73856       ['max_pooling2d_6[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_12 (Dropout)           (None, 64, 64, 128)  0           ['conv2d_25[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_26 (Conv2D)             (None, 64, 64, 128)  147584      ['dropout_12[0][0]']             \n",
      "                                                                                                  \n",
      " max_pooling2d_7 (MaxPooling2D)  (None, 32, 32, 128)  0          ['conv2d_26[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_27 (Conv2D)             (None, 32, 32, 256)  295168      ['max_pooling2d_7[0][0]']        \n",
      "                                                                                                  \n",
      " dropout_13 (Dropout)           (None, 32, 32, 256)  0           ['conv2d_27[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_28 (Conv2D)             (None, 32, 32, 256)  590080      ['dropout_13[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_transpose_4 (Conv2DTran  (None, 64, 64, 128)  131200     ['conv2d_28[0][0]']              \n",
      " spose)                                                                                           \n",
      "                                                                                                  \n",
      " concatenate_4 (Concatenate)    (None, 64, 64, 256)  0           ['conv2d_transpose_4[0][0]',     \n",
      "                                                                  'conv2d_26[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_29 (Conv2D)             (None, 64, 64, 128)  295040      ['concatenate_4[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_14 (Dropout)           (None, 64, 64, 128)  0           ['conv2d_29[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_30 (Conv2D)             (None, 64, 64, 128)  147584      ['dropout_14[0][0]']             \n",
      "                                                                                                  \n",
      " conv2d_transpose_5 (Conv2DTran  (None, 128, 128, 64  32832      ['conv2d_30[0][0]']              \n",
      " spose)                         )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_5 (Concatenate)    (None, 128, 128, 12  0           ['conv2d_transpose_5[0][0]',     \n",
      "                                8)                                'conv2d_24[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_31 (Conv2D)             (None, 128, 128, 64  73792       ['concatenate_5[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_15 (Dropout)           (None, 128, 128, 64  0           ['conv2d_31[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_32 (Conv2D)             (None, 128, 128, 64  36928       ['dropout_15[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_transpose_6 (Conv2DTran  (None, 256, 256, 32  8224       ['conv2d_32[0][0]']              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " spose)                         )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_6 (Concatenate)    (None, 256, 256, 64  0           ['conv2d_transpose_6[0][0]',     \n",
      "                                )                                 'conv2d_22[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_33 (Conv2D)             (None, 256, 256, 32  18464       ['concatenate_6[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_16 (Dropout)           (None, 256, 256, 32  0           ['conv2d_33[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_34 (Conv2D)             (None, 256, 256, 32  9248        ['dropout_16[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_transpose_7 (Conv2DTran  (None, 512, 512, 16  2064       ['conv2d_34[0][0]']              \n",
      " spose)                         )                                                                 \n",
      "                                                                                                  \n",
      " concatenate_7 (Concatenate)    (None, 512, 512, 32  0           ['conv2d_transpose_7[0][0]',     \n",
      "                                )                                 'conv2d_20[0][0]']              \n",
      "                                                                                                  \n",
      " conv2d_35 (Conv2D)             (None, 512, 512, 16  4624        ['concatenate_7[0][0]']          \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " dropout_17 (Dropout)           (None, 512, 512, 16  0           ['conv2d_35[0][0]']              \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_36 (Conv2D)             (None, 512, 512, 16  2320        ['dropout_17[0][0]']             \n",
      "                                )                                                                 \n",
      "                                                                                                  \n",
      " conv2d_37 (Conv2D)             (None, 512, 512, 2)  34          ['conv2d_36[0][0]']              \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 1,941,122\n",
      "Trainable params: 1,941,122\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[1;32mIn [19]\u001b[0m, in \u001b[0;36m<cell line: 111>\u001b[1;34m()\u001b[0m\n\u001b[0;32m    100\u001b[0m model_checkpoint_callback \u001b[38;5;241m=\u001b[39m callbacks\u001b[38;5;241m.\u001b[39mModelCheckpoint(\n\u001b[0;32m    101\u001b[0m     filepath\u001b[38;5;241m=\u001b[39mcheckpoint_filepath,\n\u001b[0;32m    102\u001b[0m     save_weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    103\u001b[0m     monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_mean_io_u\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    104\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    105\u001b[0m     save_best_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m#If starting with pre-trained weights. \u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m#model.load_weights('???.hdf5')\u001b[39;00m\n\u001b[1;32m--> 111\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train_cat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    112\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    113\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    114\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m400\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    115\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test_cat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m    116\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;66;43;03m#class_weight=class_weights,\u001b[39;49;00m\n\u001b[0;32m    117\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mmodel_checkpoint_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    118\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;66;03m# model.save('unet_shapes_detect.hdf5')\u001b[39;00m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpickle\u001b[39;00m \n",
      "File \u001b[1;32m~\\Anaconda3\\envs\\yolo\\lib\\site-packages\\keras-2.9.0-py3.8.egg\\keras\\utils\\traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[0;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "\n",
    "# from tensorflow.compat.v1 import ConfigProto\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.compat.v1 import InteractiveSession\n",
    "# # import tensorflow.keras.backend as K\n",
    "\n",
    "IMG_HEIGHT = 512#X_train.shape[1]\n",
    "IMG_WIDTH  = 512#X_train.shape[2]\n",
    "IMG_CHANNELS = 3#X_train.shape[3]\n",
    "\n",
    "# def fix_gpu():\n",
    "#     config = ConfigProto()\n",
    "#     config.gpu_options.allow_growth = True\n",
    "#     session = InteractiveSession(config=config)\n",
    "    \n",
    "# fix_gpu()\n",
    "\n",
    "# def binary_tversky_coef(y_true: tf.Tensor, y_pred: tf.Tensor, beta: float, smooth: float = 1.):\n",
    "#     \"\"\"\n",
    "#     Tversky coefficient is a generalization of the Dice's coefficient. It adds an extra weight () to false positives\n",
    "#     and false negatives:\n",
    "#         TC(p, p) = p*p/[p*p + *(1-p)*p + (1-)*p*(1-p)]\n",
    "#     When =1/2, Tversky coefficient is equal to the Dice's coefficient:\n",
    "#         TL(p, p) = p*p/[p*p + (1/2)*(1-p)*p + (1-(1/2))*p*(1-p)]\n",
    "#         = p*p/[p*p + (1/2)*p - (1/2)*p*p + (1/2)*p*(1-p)]\n",
    "#         = p*p/[p*p + (1/2)*p - (1/2)*p*p + (1/2)*p - (1/2)*p*p)]\n",
    "#         = p*p/[p*p - p*p + (1/2)*p + (1/2)*p]\n",
    "#         = p*p/[(1/2)*p + (1/2)*p]\n",
    "#         = p*p/[(1/2)*(p+p)]\n",
    "#         = 2*p*p/(p+p)\n",
    "#     :param y_true: True masks (tf.Tensor, shape=(<BATCH_SIZE>, <IMAGE_HEIGHT>, <IMAGE_WIDTH>, 1))\n",
    "#     :param y_pred: Predicted masks (tf.Tensor, shape=(<BATCH_SIZE>, <IMAGE_HEIGHT>, <IMAGE_WIDTH>, 1))\n",
    "#     :param beta: Weight coefficient (float)\n",
    "#     :param smooth: Smoothing factor (float, default = 1.)\n",
    "#     :return: Tversky coefficient (tf.Tensor, shape=(<BATCH_SIZE>, <IMAGE_HEIGHT>, <IMAGE_WIDTH>))\n",
    "#     \"\"\"\n",
    "#     axis_to_reduce = range(1, K.ndim(y_pred))  # All axis but first (batch)\n",
    "#     numerator = K.sum(y_true * y_pred, axis=axis_to_reduce)  # p*p\n",
    "#     denominator = y_true * y_pred + beta * (1 - y_true) * y_pred + (1 - beta) * y_true * (1 - y_pred)  # p*p + *(1-p)*p + (1-)*p*(1-p)\n",
    "#     denominator = K.sum(denominator, axis=axis_to_reduce)\n",
    "\n",
    "#     return (numerator + smooth) / (denominator + smooth)  # (p*p + smooth)/[p*p + *(1-p)*p + (1-)*p*(1-p) + smooth]\n",
    "\n",
    "\n",
    "# def binary_weighted_dice_crossentropy_loss(smooth: float = 1.,\n",
    "#                                            beta: float = 0.5):\n",
    "#     \"\"\"\n",
    "#     Weighted Dice cross entropy combination loss is a weighted combination between Dice's coefficient loss and\n",
    "#     binary cross entropy:\n",
    "#         DL(p, p) = 1 - (2*p*p+smooth)/(p+p+smooth)\n",
    "#         CE(p, p) = - [p*log(p + 1e-7) + (1-p)*log(1-p + 1e-7)]\n",
    "#         WDCE(p, p) = weight*DL + (1-weight)*CE\n",
    "#                    = weight*[1 - (2*p*p+smooth)/(p+p+smooth)] - (1-weight)*[p*log(p + 1e-7) + (1-p)*log(1-p + 1e-7)]\n",
    "#     Used as loss function for binary image segmentation with one-hot encoded masks.\n",
    "#     :param smooth: Smoothing factor (float, default=1.)\n",
    "#     :param beta: Loss weight coefficient (float, default=0.5)\n",
    "#     :return: Dice cross entropy combination loss (Callable[[tf.Tensor, tf.Tensor], tf.Tensor])\n",
    "#     \"\"\"\n",
    "#     assert 0. <= beta <= 1., \"Loss weight has to be between 0.0 and 1.0\"\n",
    "\n",
    "#     def loss(y_true: tf.Tensor, y_pred: tf.Tensor) -> tf.Tensor:\n",
    "#         \"\"\"\n",
    "#         Compute the Dice cross entropy combination loss.\n",
    "#         :param y_true: True masks (tf.Tensor, shape=(<BATCH_SIZE>, <IMAGE_HEIGHT>, <IMAGE_WIDTH>, 1))\n",
    "#         :param y_pred: Predicted masks (tf.Tensor, shape=(<BATCH_SIZE>, <IMAGE_HEIGHT>, <IMAGE_WIDTH>, 1))\n",
    "#         :return: Dice cross entropy combination loss (tf.Tensor, shape=(<BATCH_SIZE>,))\n",
    "#         \"\"\"\n",
    "#         cross_entropy = K.binary_crossentropy(target=y_true, output=y_true)\n",
    "\n",
    "#         # Average over each data point/image in batch\n",
    "#         axis_to_reduce = range(1, K.ndim(cross_entropy))\n",
    "#         cross_entropy = K.mean(x=cross_entropy, axis=axis_to_reduce)\n",
    "\n",
    "#         dice_coefficient = binary_tversky_coef(y_true=y_true, y_pred=y_pred, beta=0.5, smooth=smooth)\n",
    "\n",
    "#         return beta*(1. - dice_coefficient) + (1. - beta)*cross_entropy\n",
    "\n",
    "#     return loss\n",
    "\n",
    "# def balanced_cross_entropy(beta = 0.3):\n",
    "#     def loss(y_true, y_pred):\n",
    "#         weight_a = beta * tf.cast(y_true, tf.float32)\n",
    "#         weight_b = (1 - beta) * tf.cast(1 - y_true, tf.float32)\n",
    "\n",
    "#         o = (tf.math.log1p(tf.exp(-tf.abs(y_pred))) + tf.nn.relu(-y_pred)) * (weight_a + weight_b) + y_pred * weight_b\n",
    "#         return tf.reduce_mean(o)\n",
    "\n",
    "#     return loss\n",
    "\n",
    "def get_model():\n",
    "    return multi_unet_model(n_classes=n_classes, IMG_HEIGHT=IMG_HEIGHT, IMG_WIDTH=IMG_WIDTH, IMG_CHANNELS=IMG_CHANNELS) #balanced_cross_entropy(0.3)\n",
    "\n",
    "model = get_model()###################################################'categorical_crossentropy'\n",
    "model.compile(optimizer=keras.optimizers.Adam(learning_rate = 0.0001), loss=\"categorical_crossentropy\", \n",
    "              metrics=['accuracy',keras.metrics.MeanIoU(num_classes=n_classes)])\n",
    "model.summary()\n",
    "\n",
    "model_folder_name = \"unet_weight_500\" \n",
    "\n",
    "checkpoint_filepath = \"D:\\\\01_Thesis\\\\learning\\\\enpal\\\\\" + model_folder_name + \"\\\\checkpoint\"\n",
    "model_checkpoint_callback = callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    save_weights_only=False,\n",
    "    monitor='val_mean_io_u',\n",
    "    mode='max',\n",
    "    save_best_only=True)\n",
    "\n",
    "\n",
    "#If starting with pre-trained weights. \n",
    "#model.load_weights('???.hdf5')\n",
    "\n",
    "history = model.fit(X_train, y_train_cat, \n",
    "                    batch_size = 4, \n",
    "                    verbose=1, \n",
    "                    epochs=400, \n",
    "                    validation_data=(X_test, y_test_cat), \n",
    "                    #class_weight=class_weights,\n",
    "                     callbacks=[model_checkpoint_callback],\n",
    "                    shuffle=False)\n",
    "# model.save('unet_shapes_detect.hdf5')\n",
    "                    \n",
    "import pickle \n",
    "with open( model_folder_name +'with_border_trainstats', 'wb') as file_pi:\n",
    "    pickle.dump(history.history, file_pi)\n",
    "# model.save('sandstone_50_epochs_catXentropy_acc_with_weights.hdf5')\n",
    "###########################################################\n",
    "## Evaluate the model\n",
    "## \tevaluate model\n",
    "_, acc = model.evaluate(X_test, y_test_cat)\n",
    "print(\"Accuracy is = \", (acc * 100.0), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the training and validation accuracy and loss at each epoch\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(loss) + 1)\n",
    "plt.plot(epochs, loss, 'y', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acc = history.history['accuracy']\n",
    "# val_acc = history.history['val_accuracy']\n",
    "\n",
    "# acc = history.history['mean_io_u']\n",
    "val_acc = history.history['val_mean_io_u']\n",
    "\n",
    "# plt.plot(epochs, acc, 'y', label='Training mean_iou')\n",
    "# plt.plot(epochs, val_acc, 'r', label='Validation mean_iou')\n",
    "# plt.title('Training and validation mean_iou')\n",
    "# plt.xlabel('Epochs')\n",
    "# plt.ylabel('Accuracy')\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "val_acc\n",
    "max_value = max(val_acc)\n",
    "max_index = val_acc.index(max_value)\n",
    "print(max_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"D:\\\\01_Thesis\\\\learning\\\\enpal\\\\\" + model_folder_name + \"\\\\checkpoint\", compile=False)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "pred_img_name = \"5.jpeg\"\n",
    "pred_path = r\"D:\\\\01_Thesis\\\\unet\\\\vishnu\\\\test\\\\\"\n",
    "pred_path = pred_path + pred_img_name\n",
    "act_img = cv2.imread(pred_path, 1) \n",
    "# img = np.pad(act_img,((103,103), (3,3), (0,0)),\"constant\",constant_values=(0))\n",
    "pred_img = np.reshape(img,(1,256,256,3))\n",
    "pred_img = np.expand_dims(pred_img, axis=-1)\n",
    "pred_img = normalize(pred_img, axis=1)\n",
    "pred_img = np.reshape(pred_img,(1,256,256,3))\n",
    "\n",
    "pred = model.predict(pred_img)\n",
    "out_img = np.zeros((256,256,3))\n",
    "out_img[...,0:2] = np.reshape(pred,(256,256,2))\n",
    "# class_1 = np.zeros((656,656,3))\n",
    "# class_2 = np.zeros((656,656,3))\n",
    "# class_1[:,:,0:2] = out_img[:,:,0:2].copy()\n",
    "# class_1[:,:,2] = out_img[:,:,3].copy()\n",
    "\n",
    "# class_2[:,:,0:2] = out_img[:,:,0:2].copy()\n",
    "# class_2[:,:,2] = out_img[:,:,3].copy()\n",
    "# class_1[:,:,0] = 0\n",
    "# class_1[:,:,2] = 0\n",
    "# class_2[:,:,0] = 0\n",
    "# class_2[:,:,1] = 0\n",
    "plt.imshow(class_1)\n",
    "cv2.imwrite(\"D:\\\\01_Thesis\\\\unet\\\\vishnu\\\\test\\\\pred_\" +pred_img_name,out_img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import colors, cm, pyplot as plt\n",
    "\n",
    "img_no = 4\n",
    "img = X_do_not_use[img_no]\n",
    "img = np.reshape(img,(1,256,256,3))\n",
    "print(img.shape)\n",
    "# img = normalize(img)\n",
    "\n",
    "pred = model.predict(img)\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.subplot(131)\n",
    "#img = np.reshape(X_do_not_use[img_no], (656, 656,3))\n",
    "img = np.squeeze(X_do_not_use[img_no]*50, axis=-1)\n",
    "#norm = colors.LogNorm(img.mean() + 0.5 * img.std(), img.max(), clip='True')\n",
    "plt.imshow(img, origin=\"lower\")\n",
    "pred = np.reshape(pred, (256, 256,2))\n",
    "pred = pred[:,:,1] * 255\n",
    "print(pred)\n",
    "plt.imshow(np.reshape(X_do_not_use[img_no], (256, 256,3))*100)\n",
    "plt.subplot(132)\n",
    "plt.imshow(pred, cmap='gray')\n",
    "plt.subplot(133)\n",
    "plt.imshow(np.reshape(y_do_not_use[img_no], (256, 256)), cmap='gray')\n",
    "plt.show()\n",
    "#print(X_train[image_number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import colors, cm, pyplot as plt\n",
    "import time\n",
    "for img_no in range(X_do_not_use.shape[0]):\n",
    "\n",
    "    img = X_do_not_use[img_no]\n",
    "    img = np.reshape(img,(1,256,256,3))\n",
    "    print(img.shape)\n",
    "    # img = normalize(img)\n",
    "\n",
    "    pred = model.predict(img)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(131)\n",
    "    #img = np.reshape(X_do_not_use[img_no], (656, 656,3))\n",
    "    img = np.squeeze(X_do_not_use[img_no]*50, axis=-1)\n",
    "    #norm = colors.LogNorm(img.mean() + 0.5 * img.std(), img.max(), clip='True')\n",
    "    plt.imshow(img, origin=\"lower\")\n",
    "    pred = np.reshape(pred, (256, 256,2))\n",
    "    pred = pred[:,:,1]\n",
    "    plt.imshow(np.reshape(X_do_not_use[img_no], (256, 256,3))*100)\n",
    "    plt.subplot(132)\n",
    "    plt.imshow(pred, cmap='gray')\n",
    "    plt.subplot(133)\n",
    "    plt.imshow(np.reshape(y_do_not_use[img_no], (256, 256)), cmap='gray')\n",
    "    plt.show()\n",
    "    time.sleep(1)\n",
    "    #print(X_train[image_number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict images in folder\n",
    "from tqdm import tqdm\n",
    "folder = r\"D:\\\\01_Thesis\\\\learning\\\\enpal\\\\train\\\\image_250\\\\New_folder\\\\\"\n",
    "images = glob.glob(folder + \"*.tif\")\n",
    "save_path = r\"D:\\\\01_Thesis\\\\learning\\\\enpal\\\\train\\\\prediction\\\\\"\n",
    "# print(images)\n",
    "for image in tqdm(images):\n",
    "    file_name = image.split(\"\\\\\")[-1]\n",
    "    file_name = file_name.split(\".\")[0]\n",
    "    print(file_name)\n",
    "    img = cv2.imread(image, 1)\n",
    "#     img = cv2.resize(img, (SIZE_Y, SIZE_X), interpolation = cv2.INTER_NEAREST)\n",
    "    img = np.pad(img,((3,3), (3,3), (0,0)),\"constant\",constant_values=(0))\n",
    "    img = np.reshape(img,(1,256,256,3))\n",
    "    img = np.expand_dims(img, axis=-1)\n",
    "    img = normalize(img, axis=1)\n",
    "    img = np.reshape(img,(1,256,256,3))\n",
    "    pred = model.predict(img)\n",
    "    pred = np.reshape(pred, (256, 256,2))\n",
    "    pred = pred[:,:,1]\n",
    "#     pred = model.predict(img)\n",
    "#     pred = np.reshape(pred, (256, 256,6))\n",
    "#     temp = np.zeros((256,256,3))\n",
    "# #     temp[:,:,0] = pred[:,:,0]\n",
    "#     temp[:,:,0] = pred[:,:,0]\n",
    "#     temp[:,:,1] = pred[:,:,1]\n",
    "#     temp[:,:,1] = pred[:,:,2]\n",
    "    save_img = save_path + file_name + \"_Pred.tif\"\n",
    "    cv2.imwrite(save_img,pred*225)\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(pred).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_path = \"D:\\\\01_Thesis\\\\unet\\\\train\\\\masks\\\\model_51.jpeg\"\n",
    "# model_out = np.reshape(pred, (656, 656,3))\n",
    "# cv2.imwrite(save_path,markers_8u[103:103+450,:]*125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "pred_img_name = \"23.jpeg\"\n",
    "pred_path = r\"D:\\\\01_Thesis\\\\unet\\\\train\\\\images\\\\\"\n",
    "pred_path = pred_path + pred_img_name\n",
    "act_img = cv2.imread(pred_path, 1) \n",
    "act_img = cv2.resize(img, (SIZE_Y, SIZE_X), interpolation = cv2.INTER_NEAREST)\n",
    "# img = np.pad(act_img,((103,103), (3,3), (0,0)),\"constant\",constant_values=(0))\n",
    "pred_img = np.reshape(img,(1,256,256,3))\n",
    "pred_img = np.expand_dims(pred_img, axis=-1)\n",
    "pred_img = normalize(pred_img, axis=1)\n",
    "pred_img = np.reshape(pred_img,(1,256,256,3))\n",
    "\n",
    "pred = model.predict(pred_img)\n",
    "out_img = np.zeros((256,256,3))\n",
    "out_img[...,0:2] = np.reshape(pred,(256,256,2))\n",
    "# class_1 = np.zeros((656,656,0))\n",
    "# class_2 = np.zeros((656,656,1))\n",
    "# class_1[:,:,0:2] = out_img[:,:,0:2].copy()\n",
    "# class_1[:,:,2] = out_img[:,:,3].copy()\n",
    "\n",
    "# class_2[:,:,0:2] = out_img[:,:,0:2].copy()\n",
    "# class_2[:,:,2] = out_img[:,:,3].copy()\n",
    "# class_1[:,:,0] = 0\n",
    "# class_1[:,:,2] = 0\n",
    "# class_2[:,:,0] = 0\n",
    "# class_2[:,:,1] = 0\n",
    "plt.imshow(out_img)\n",
    "# cv2.imwrite(\"D:\\\\01_Thesis\\\\unet\\\\train\\\\masks\\\\model_68.jpeg\",class_1[103:103+450,:]*125)\n",
    "cv2.imwrite(\"D:\\\\01_Thesis\\\\unet\\\\thesis_doc\\\\dist_model_1_21.jpeg\",out_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# info = np.iinfo(class_1.dtype) # Get the information of the incoming image type\n",
    "# data = class_1.astype(np.float64) / info.max # normalize the data to 0 - 1\n",
    "data = 255 * class_1 # Now scale by 255\n",
    "# data = img\n",
    "img = data.astype(np.uint8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a kernel that we will use to sharpen our image\n",
    "# an approximation of second derivative, a quite strong kernel\n",
    "#class_1 = cv2.imread(\"D:\\\\01_Thesis\\\\unet\\\\train\\\\masks\\\\model_68.jpeg\", 1)\n",
    "kernel = np.array([[1, 1, 1], [1, -8, 1], [1, 1, 1]], dtype=np.float32)\n",
    "# do the laplacian filtering as it is\n",
    "# well, we need to convert everything in something more deeper then CV_8U\n",
    "# because the kernel has some negative values,\n",
    "# and we can expect in general to have a Laplacian image with negative values\n",
    "# BUT a 8bits unsigned int (the one we are working with) can contain values from 0 to 255\n",
    "# so the possible negative number will be truncated\n",
    "# class_1 = class_1 * 125\n",
    "# class_1 = class_1.astype('uint8')\n",
    "imgLaplacian = cv2.filter2D(img, cv2.CV_32F, kernel)\n",
    "sharp = np.float32(img)\n",
    "imgResult = sharp - imgLaplacian\n",
    "# convert back to 8bits gray scale\n",
    "imgResult = np.clip(imgResult, 0, 255)\n",
    "imgResult = imgResult.astype('uint8')\n",
    "imgLaplacian = np.clip(imgLaplacian, 0, 255)\n",
    "imgLaplacian = np.uint8(imgLaplacian)\n",
    "plt.imshow( imgLaplacian)\n",
    "plt.imshow(imgResult)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create binary image from source image\n",
    "bw = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "_, bw = cv2.threshold(bw, 40, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "plt.imshow(bw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the distance transform algorithm\n",
    "dist = cv2.distanceTransform(bw, cv2.DIST_L2, 3)\n",
    "# Normalize the distance image for range = {0.0, 1.0}\n",
    "# so we can visualize and threshold it\n",
    "cv2.normalize(dist, dist, 0, 1.0, cv2.NORM_MINMAX)\n",
    "plt.imshow( dist)\n",
    "cv2.imwrite(\"D:\\\\01_Thesis\\\\unet\\\\thesis_doc\\\\dist_model_1_12.jpeg\",dist[103:103+450,:]*125)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold to obtain the peaks\n",
    "# This will be the markers for the foreground objects\n",
    "_, dist = cv2.threshold(dist, 0.4, 1.0, cv2.THRESH_BINARY)\n",
    "# Dilate a bit the dist image\n",
    "kernel1 = np.ones((3,3), dtype=np.uint8)\n",
    "dist = cv2.dilate(dist, kernel1)\n",
    "plt.imshow( dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the CV_8U version of the distance image\n",
    "# It is needed for findContours()\n",
    "dist_8u = dist.astype('uint8')\n",
    "# Find total markers\n",
    "contours, _ = cv2.findContours(dist_8u, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "# Create the marker image for the watershed algorithm\n",
    "markers = np.zeros(dist.shape, dtype=np.int32)\n",
    "# Draw the foreground markers\n",
    "for i in range(len(contours)):\n",
    "    cv2.drawContours(markers, contours, i, (i+1), -1)\n",
    "# Draw the background marker\n",
    "cv2.circle(markers, (5,5), 3, (255,255,255), -1)\n",
    "markers_8u = (markers * 10).astype('uint8')\n",
    "plt.imshow(markers_8u)\n",
    "cont_det_image = cv2.cvtColor(markers_8u[103:103+450,:,], cv2.COLOR_GRAY2RGB)\n",
    "cont_det_image[:,:,2] = cont_det_image[:,:,2] + 60\n",
    "cont_det_image[:,:,1] = cont_det_image[:,:,1] + 120\n",
    "cont_det_image[:,:,0] = cont_det_image[:,:,0] + 20\n",
    "cv2.imwrite(\"D:\\\\01_Thesis\\\\unet\\\\thesis_doc\\\\contour_dect_model_1_23.jpeg\",markers_8u[103:103+450,:,] + 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform the watershed algorithm\n",
    "cv2.watershed(imgResult, markers)\n",
    "#mark = np.zeros(markers.shape, dtype=np.uint8)\n",
    "mark = markers.astype('uint8')\n",
    "mark = cv2.bitwise_not(mark)\n",
    "# uncomment this if you want to see how the mark\n",
    "# image looks like at that point\n",
    "#cv.imshow('Markers_v2', mark)\n",
    "# Generate random colors\n",
    "colors = []\n",
    "for contour in contours:\n",
    "    colors.append((random.randint(0,256), random.randint(0,256), random.randint(0,256)))\n",
    "# Create the result image\n",
    "dst = np.zeros((markers.shape[0], markers.shape[1], 3), dtype=np.uint8)\n",
    "# Fill labeled objects with random colors\n",
    "for i in range(markers.shape[0]):\n",
    "    for j in range(markers.shape[1]):\n",
    "        index = markers[i,j]\n",
    "        if index > 0 and index <= len(contours):\n",
    "            dst[i,j,:] = colors[index-1]\n",
    "# Visualize the final image\n",
    "plt.imshow( dst)\n",
    "cv2.imwrite(\"D:\\\\01_Thesis\\\\unet\\\\thesis_doc\\\\wshed_model_2_23.jpeg\",dst[103:103+450,:]*125)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "re_cont = []\n",
    "temp = []\n",
    "for cont in contours:\n",
    "    for pts in cont:\n",
    "        temp.append(pts[0])\n",
    "    re_cont.append(temp)\n",
    "    temp = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_cont = []\n",
    "\n",
    "for cnt in re_cont:\n",
    "    fg = 0\n",
    "    for pts in cnt:\n",
    "        if pts[1] > 500 or pts[1] < 180 :\n",
    "            fg = 1\n",
    "    if fg == 0:\n",
    "        filt_cont.append(cnt)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blank = np.zeros((656,656,3))\n",
    "isClosed = True\n",
    "  \n",
    "# Blue color in BGR\n",
    "color = (255, 255, 255)\n",
    "  \n",
    "# Line thickness of 2 px\n",
    "thickness = 2\n",
    "for cnt in filt_cont:\n",
    "    blank = cv2.polylines(blank, [np.array(cnt)], \n",
    "                      isClosed, color, thickness)\n",
    "    \n",
    "plt.imshow(blank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# font\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "  \n",
    "# org\n",
    "org = (50, 50)\n",
    "  \n",
    "# fontScale\n",
    "fontScale = 1\n",
    "   \n",
    "# Blue color in BGR\n",
    "color = (255, 255, 255)\n",
    "  \n",
    "# Line thickness of 2 px\n",
    "thickness = 2\n",
    "   \n",
    "# # Using cv2.putText() method\n",
    "# image = cv2.putText(blank, 'OpenCV', org, font, \n",
    "#                    fontScale, color, thickness, cv2.LINE_AA)\n",
    "\n",
    "c = 1\n",
    "for cnt in filt_cont:\n",
    "    \n",
    "    arr = np.array(cnt)\n",
    "    x = np.max(arr[...,0])\n",
    "    y = np.max(arr[...,1]) -103\n",
    "    blank = cv2.putText(act_img, str(c), (x,y), font, \n",
    "                   fontScale, color, thickness, cv2.LINE_AA)\n",
    "    c = c +1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(cv2.cvtColor(blank, cv2.COLOR_BGR2RGB ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"D:\\\\01_Thesis\\\\unet\\\\thesis_doc\\\\inst_model_2_23.jpeg.jpeg\"\n",
    "# model_out = np.reshape(pred, (656, 656,3))\n",
    "cv2.imwrite(save_path,blank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Border_filled based detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For loop images in folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict images in folder\n",
    "from tqdm import tqdm\n",
    "folder = r\"C:\\Users\\bakthavatchalam\\Desktop\\unet\\border_filled_data\\train\\\\\"\n",
    "images = glob.glob(folder + \"*.jpeg\")\n",
    "save_path = r\"C:\\Users\\bakthavatchalam\\Desktop\\unet\\predicted\\\\\"\n",
    "# print(images)\n",
    "classes = [1,2]\n",
    "for image in tqdm(images):\n",
    "    file_name = image.split(\"\\\\\")[-1]\n",
    "    read_img = cv2.imread(image, 1)\n",
    "    act_img = read_img.copy()\n",
    "    read_img = np.pad(read_img,((103,103), (3,3), (0,0)),\"constant\",constant_values=(0))\n",
    "    read_img = np.reshape(read_img,(1,656,656,3))\n",
    "    read_img = np.expand_dims(read_img, axis=-1)\n",
    "    read_img = normalize(read_img, axis=1)\n",
    "    read_img = np.reshape(read_img,(1,656,656,3))\n",
    "    pred = model.predict(read_img)\n",
    "    pred = np.reshape(pred, (656, 656,5))\n",
    "\n",
    "    \n",
    "    for cls in classes:\n",
    "        \n",
    "        temp_cls = np.zeros((656,656,3))\n",
    "        if cls == 1:\n",
    "            temp_cls[:,:,1] = pred[:,:,1]\n",
    "        \n",
    "        if cls == 2:\n",
    "            temp_cls[:,:,2] = pred[:,:,3]\n",
    "        data = 255 * temp_cls\n",
    "        \n",
    "        img = data.astype(np.uint8)\n",
    "        kernel = np.array([[1, 1, 1], [1, -8, 1], [1, 1, 1]], dtype=np.float32)\n",
    "        imgLaplacian = cv2.filter2D(img, cv2.CV_32F, kernel)\n",
    "        sharp = np.float32(img)\n",
    "        imgResult = sharp - imgLaplacian\n",
    "        imgResult = np.clip(imgResult, 0, 255)\n",
    "        imgResult = imgResult.astype('uint8')\n",
    "        imgLaplacian = np.clip(imgLaplacian, 0, 255)\n",
    "        imgLaplacian = np.uint8(imgLaplacian)\n",
    "        bw = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        _, bw = cv2.threshold(bw, 40, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "        dist = cv2.distanceTransform(bw, cv2.DIST_L2, 3)\n",
    "        cv2.normalize(dist, dist, 0, 1.0, cv2.NORM_MINMAX)\n",
    "        _, dist = cv2.threshold(dist, 0.4, 1.0, cv2.THRESH_BINARY)\n",
    "        kernel1 = np.ones((3,3), dtype=np.uint8)\n",
    "        dist = cv2.dilate(dist, kernel1)\n",
    "        dist_8u = dist.astype('uint8')\n",
    "        # Find total markers\n",
    "        contours, _ = cv2.findContours(dist_8u, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        # Create the marker image for the watershed algorithm\n",
    "        markers = np.zeros(dist.shape, dtype=np.int32)\n",
    "        # Draw the foreground markers\n",
    "        for i in range(len(contours)):\n",
    "            cv2.drawContours(markers, contours, i, (i+1), -1)\n",
    "        # Draw the background marker\n",
    "        cv2.circle(markers, (5,5), 3, (255,255,255), -1)\n",
    "        markers_8u = (markers * 10).astype('uint8')\n",
    "        cv2.watershed(imgResult, markers)\n",
    "        #mark = np.zeros(markers.shape, dtype=np.uint8)\n",
    "        mark = markers.astype('uint8')\n",
    "        mark = cv2.bitwise_not(mark)\n",
    "        # uncomment this if you want to see how the mark\n",
    "        # image looks like at that point\n",
    "        #cv.imshow('Markers_v2', mark)\n",
    "        # Generate random colors\n",
    "        colors = []\n",
    "        for contour in contours:\n",
    "            colors.append((random.randint(0,256), random.randint(0,256), random.randint(0,256)))\n",
    "        # Create the result image\n",
    "        dst = np.zeros((markers.shape[0], markers.shape[1], 3), dtype=np.uint8)\n",
    "        # Fill labeled objects with random colors\n",
    "        for i in range(markers.shape[0]):\n",
    "            for j in range(markers.shape[1]):\n",
    "                index = markers[i,j]\n",
    "                if index > 0 and index <= len(contours):\n",
    "                    dst[i,j,:] = colors[index-1]\n",
    "                    \n",
    "        re_cont = []\n",
    "        temp = []\n",
    "        for cont in contours:\n",
    "            for pts in cont:\n",
    "                temp.append(pts[0])\n",
    "            re_cont.append(temp)\n",
    "            temp = []\n",
    "            \n",
    "        filt_cont = []\n",
    "\n",
    "        for cnt in re_cont:\n",
    "            fg = 0\n",
    "            for pts in cnt:\n",
    "                if pts[1] > 500 or pts[1] < 180 :\n",
    "                    fg = 1\n",
    "            if fg == 0:\n",
    "                filt_cont.append(cnt)\n",
    "                \n",
    "        blank = np.zeros((656,656,3))\n",
    "        isClosed = True\n",
    "\n",
    "        # Blue color in BGR\n",
    "        color = (255, 255, 255)\n",
    "\n",
    "        # Line thickness of 2 px\n",
    "        thickness = 2\n",
    "        for cnt in filt_cont:\n",
    "            blank = cv2.polylines(blank, [np.array(cnt)], \n",
    "                              isClosed, color, thickness)\n",
    "            \n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "  \n",
    "\n",
    "        org = (50, 50)\n",
    "\n",
    "        # fontScale\n",
    "        fontScale = 1\n",
    "\n",
    "        # Blue color in BGR\n",
    "        if cls == 1:\n",
    "            color = (255, 255, 255)\n",
    "            \n",
    "        if cls == 2:\n",
    "            color = (0, 0, 0)\n",
    "\n",
    "        # Line thickness of 2 px\n",
    "        thickness = 2\n",
    "\n",
    "        # # Using cv2.putText() method\n",
    "        # image = cv2.putText(blank, 'OpenCV', org, font, \n",
    "        #                    fontScale, color, thickness, cv2.LINE_AA)\n",
    "\n",
    "        c = 1\n",
    "        for cnt in filt_cont:\n",
    "\n",
    "            arr = np.array(cnt)\n",
    "            x = np.max(arr[...,0])\n",
    "            y = np.max(arr[...,1]) -103\n",
    "            blank = cv2.putText(act_img, str(c), (x,y), font, \n",
    "                           fontScale, color, thickness, cv2.LINE_AA)\n",
    "            c = c +1\n",
    "    \n",
    "\n",
    "        cv2.imwrite(\"C:\\\\Users\\\\bakthavatchalam\\\\Desktop\\\\unet\\\\pred_numbered\\\\\" + file_name,blank)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Centre or picking place determination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Predict images in folder\n",
    "from tqdm import tqdm\n",
    "folder = r\"C:\\Users\\bakthavatchalam\\Desktop\\unet\\border_filled_data\\train\\\\\"\n",
    "images = glob.glob(folder + \"*.jpeg\")\n",
    "# images = [images[0]]\n",
    "print(images)\n",
    "save_path = r\"C:\\Users\\bakthavatchalam\\Desktop\\unet\\predicted\\\\\"\n",
    "# print(images)\n",
    "classes = [1,2]\n",
    "for image in tqdm(images):\n",
    "    file_name = image.split(\"\\\\\")[-1]\n",
    "    print(file_name)\n",
    "    read_img = cv2.imread(image, 1)\n",
    "    act_img = read_img.copy()\n",
    "    read_img = np.pad(read_img,((103,103), (3,3), (0,0)),\"constant\",constant_values=(0))\n",
    "    read_img = np.reshape(read_img,(1,656,656,3))\n",
    "    read_img = np.expand_dims(read_img, axis=-1)\n",
    "    read_img = normalize(read_img, axis=1)\n",
    "    read_img = np.reshape(read_img,(1,656,656,3))\n",
    "    pred = model.predict(read_img)\n",
    "    pred = np.reshape(pred, (656, 656,5))\n",
    "    centres_1 = []\n",
    "    centres_2 = []\n",
    "\n",
    "    \n",
    "    for cls in classes:\n",
    "        \n",
    "        temp_cls = np.zeros((656,656,3))\n",
    "        if cls == 1:\n",
    "            temp_cls[:,:,1] = pred[:,:,1]\n",
    "        \n",
    "        if cls == 2:\n",
    "            temp_cls[:,:,2] = pred[:,:,3]\n",
    "        data = 255 * temp_cls\n",
    "        \n",
    "        img = data.astype(np.uint8)\n",
    "        kernel = np.array([[1, 1, 1], [1, -8, 1], [1, 1, 1]], dtype=np.float32)\n",
    "        imgLaplacian = cv2.filter2D(img, cv2.CV_32F, kernel)\n",
    "        sharp = np.float32(img)\n",
    "        imgResult = sharp - imgLaplacian\n",
    "        imgResult = np.clip(imgResult, 0, 255)\n",
    "        imgResult = imgResult.astype('uint8')\n",
    "        imgLaplacian = np.clip(imgLaplacian, 0, 255)\n",
    "        imgLaplacian = np.uint8(imgLaplacian)\n",
    "        bw = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        _, bw = cv2.threshold(bw, 40, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)\n",
    "        dist = cv2.distanceTransform(bw, cv2.DIST_L2, 3)\n",
    "        cv2.normalize(dist, dist, 0, 1.0, cv2.NORM_MINMAX)\n",
    "        _, dist = cv2.threshold(dist, 0.4, 1.0, cv2.THRESH_BINARY)\n",
    "        kernel1 = np.ones((3,3), dtype=np.uint8)\n",
    "        dist = cv2.dilate(dist, kernel1)\n",
    "        dist_8u = dist.astype('uint8')\n",
    "        # Find total markers\n",
    "        contours, _ = cv2.findContours(dist_8u, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        # Create the marker image for the watershed algorithm\n",
    "        markers = np.zeros(dist.shape, dtype=np.int32)\n",
    "        # Draw the foreground markers\n",
    "        for i in range(len(contours)):\n",
    "            cv2.drawContours(markers, contours, i, (i+1), -1)\n",
    "        # Draw the background marker\n",
    "        cv2.circle(markers, (5,5), 3, (255,255,255), -1)\n",
    "        markers_8u = (markers * 10).astype('uint8')\n",
    "        cv2.watershed(imgResult, markers)\n",
    "        #mark = np.zeros(markers.shape, dtype=np.uint8)\n",
    "        mark = markers.astype('uint8')\n",
    "        mark = cv2.bitwise_not(mark)\n",
    "        # uncomment this if you want to see how the mark\n",
    "        # image looks like at that point\n",
    "        #cv.imshow('Markers_v2', mark)\n",
    "        # Generate random colors\n",
    "        colors = []\n",
    "        for contour in contours:\n",
    "            colors.append((random.randint(0,256), random.randint(0,256), random.randint(0,256)))\n",
    "        # Create the result image\n",
    "        dst = np.zeros((markers.shape[0], markers.shape[1], 3), dtype=np.uint8)\n",
    "        # Fill labeled objects with random colors\n",
    "        for i in range(markers.shape[0]):\n",
    "            for j in range(markers.shape[1]):\n",
    "                index = markers[i,j]\n",
    "                if index > 0 and index <= len(contours):\n",
    "                    dst[i,j,:] = colors[index-1]\n",
    "                    \n",
    "       # extracting contours infomation             \n",
    "        re_cont = []\n",
    "        temp = []\n",
    "        for cont in contours:\n",
    "            for pts in cont:\n",
    "                temp.append(pts[0])\n",
    "            re_cont.append(temp)\n",
    "            temp = []\n",
    "            \n",
    "        filt_cont = re_cont#[]\n",
    "\n",
    "#         for cnt in re_cont:\n",
    "#             fg = 0\n",
    "#             for pts in cnt:\n",
    "#                 if pts[1] > 500 or pts[1] < 180 :\n",
    "#                     fg = 1\n",
    "#             if fg == 0:\n",
    "#                 filt_cont.append(cnt)\n",
    "                \n",
    "        blank = np.zeros((656,656,3))\n",
    "        isClosed = True\n",
    "\n",
    "        # Blue color in BGR\n",
    "        color = (255, 255, 255)\n",
    "\n",
    "        # Line thickness of 2 px\n",
    "        thickness = 2\n",
    "        for cnt in filt_cont:\n",
    "            blank = cv2.polylines(blank, [np.array(cnt)], \n",
    "                              isClosed, color, thickness)\n",
    "            \n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "  \n",
    "\n",
    "        org = (50, 50)\n",
    "\n",
    "        # fontScale\n",
    "        fontScale = 1\n",
    "\n",
    "        # Blue color in BGR\n",
    "        if cls == 1:\n",
    "            color = (255, 255, 255)\n",
    "            \n",
    "        if cls == 2:\n",
    "            color = (0, 0, 0)\n",
    "\n",
    "        # Line thickness of 2 px\n",
    "        thickness = 2\n",
    "\n",
    "        # # Using cv2.putText() method\n",
    "        # image = cv2.putText(blank, 'OpenCV', org, font, \n",
    "        #                    fontScale, color, thickness, cv2.LINE_AA)\n",
    "        \n",
    "\n",
    "        \n",
    "        for cnt in filt_cont:\n",
    "            arr = np.array(cnt)\n",
    "            M = cv2.moments(arr)\n",
    "            if M['m00'] != 0:\n",
    "                cx = int(M['m10']/M['m00'])\n",
    "                cy = int(M['m01']/M['m00']) -103\n",
    "                cv2.putText(act_img, \"X\", (cx, cy), font, 0.5, (0, 0, 0), 2)\n",
    "                \n",
    "            if cls == 1:\n",
    "                centres_1.append((cx,cy))\n",
    "            if cls == 2:\n",
    "                centres_2.append((cx,cy))\n",
    "\n",
    "\n",
    "        c = 1\n",
    "        for cnt in filt_cont:\n",
    "\n",
    "            arr = np.array(cnt)\n",
    "            x = np.max(arr[...,0])\n",
    "            y = np.max(arr[...,1]) -103\n",
    "            blank = cv2.putText(act_img, str(c), (x,y), font, \n",
    "                           fontScale, color, thickness, cv2.LINE_AA)\n",
    "            c = c + 1\n",
    "        \n",
    "\n",
    "        cv2.imwrite(\"C:\\\\Users\\\\bakthavatchalam\\\\Desktop\\\\unet\\\\pred_numbered\\\\\" + file_name, blank)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centres_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "centres_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
